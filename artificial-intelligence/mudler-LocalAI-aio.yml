# version: "3.9"

services:
    # For a specific version:
    # localai/localai:latest-aio-cpu
    # quay.io/go-skynet/local-ai:latest-aio-cpu
    # image: localai/localai:v2.23.0-aio-cpu
    # localai/localai:v2.24.2-aio-cpu
    # quay.io/go-skynet/local-ai:v2.24.2-aio-cpu
    # For Nvidia GPUs decomment one of the following (cuda11 or cuda12):
    # image: localai/localai:v2.23.0-aio-gpu-nvidia-cuda-11
    # image: localai/localai:v2.23.0-aio-gpu-nvidia-cuda-12
    # image: localai/localai:latest-aio-gpu-nvidia-cuda-11
    # image: localai/localai:latest-aio-gpu-nvidia-cuda-12
    # quay.io/go-skynet/local-ai:latest-aio-gpu-nvidia-cuda-12
    # AMD GPU localai/localai:latest-aio-gpu-hipblas
    # inter gpu localai/localai:latest-aio-gpu-intel-f16
    # inter gpu localai/localai:latest-aio-gpu-intel-f32

  # sudo docker run -it --name local-ai-gpu-20241125 -p 8080:8080 --gpus all -v /docker-data/mudler-LocalAI-data/models:/build/models localai/localai:latest-aio-gpu-nvidia-cuda-12
  mudler-LocalAI-nvidia-gpu:
    image: localai/localai:latest-aio-gpu-nvidia-cuda-12
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:16010/readyz"]
      interval: 1m
      timeout: 20m
      retries: 5
    ports:
      - 16010:8080
    environment:
      - DEBUG=true
      # ...
    volumes:
      - mudler-LocalAI-models:/build/models:cached
    #decomment the following piece if running with Nvidia GPUs
    deploy:
      resources:
         reservations:
           devices:
             - driver: nvidia
               count: 1
               capabilities: [gpu]

volumes:
  mudler-LocalAI-models: